\chapter{Conclusions and Future Directions}
\label{sec:discussion}
\label{sec:conclusions}

We have combined high-resolution cosmological zoom simulations of massive galaxy evolution at high-redshift with $3$D Monte Carlo Lyman-$\alpha$ radiative transfer simulations to develop a model for the origin of Lyman-$\alpha$ blobs at high-redshift ($z=2-5$).
Our work considers the physics of ionization radiative transfer, cooling emission, recombination, and the impact of AGN within a cosmological context of galaxy evolution.
Our main conclusions from this work follow:
\begin{itemize}

    \item When adopting a notional luminosity-based definition for LAB formation, we find that all of our model massive galaxies go through a LAB phase at some point between $z=5-2$ (Figure~\ref{fig:luminosity_redshift}, Figure~\ref{fig:agn_recombination_collision}, Figure~\ref{fig:MUSE}, Figure~\ref{fig:rogues1}, Figure~\ref{fig:rogues2}, Figure~\ref{fig:rogues4}, Figure~\ref{fig:rogues8}).  These LABs have extended morphologies in agreement with observations.

    \item The escape fraction of Lyman-$\alpha$ photons from these objects are highly orientation-dependent, which complicate our observational understanding of the connection between Lyman-$\alpha$ blobs and massive galaxy evolution. Variations in the escape fraction with respect to line of sight can produce very large variations in an observed Ly$\alpha$ luminosity (Figure~\ref{fig:los}, Figure~\ref{fig:agn_los}).
    
    \item The formation of LABs in our model is independent of the inclusion of AGN: star formation alone is enough to drive LAB formation in massive galaxies. This said, the presence of an AGN can significantly alter the Lyman-$\alpha$ luminosity and spatial extent (Figure~\ref{fig:agn_rogues1}, Figure~\ref{fig:agn_rogues2}, Figure~\ref{fig:agn_rogues4}, Figure~\ref{fig:agn_rogues8}, Figure~\ref{fig:area_plot}, Figure~\ref{fig:agn_area_plot}).
    
    \item The presence of an AGN in an LAB may be detectable by measuring the spatial concentration of Ly$\alpha$ luminosity (Figure~\ref{fig:area_plot}, Figure~\ref{fig:skewness}).
    
    \item The observed luminosity of LABs does not scale particularly well with any individual physical property.  The reason is that the intrinsic luminosity is dependent on the temperature and ionization state of the gas, which can vary strongly with the small scale geometry of the gas distribution.  Similarly, the observed luminosity folds in the escape fraction which is a strong function of the star-gas-dust geometry, as well as the small scale clumping  (Figure~\ref{fig:luminosity_vs_temperature}).
    
\end{itemize}


\section{Comparison To Previous Models}
In the work presented here, we deliberately argued that our simulations reproduce Ly$\alpha$ blobs, but if we were so motivated we could use the results of our radiative transfer calculations to construct a solid argument that these objects are not definitely the same class of object as observed blobs.
Such an argument would focus on surface brightness profiles, and the impact of AGN on the spatial extent of the Ly$\alpha$ surface brightness in our images.
We are not seriously proposing that the previous work was arguing in poor faith, but instead that this area of research is sufficiently ambiguious that given even our modestly-sized dataset there is sufficient data to argue for whichever outcome is most convenient for the original aims of the project.
In this and many other works we set out to learn about Ly$\alpha$ blobs so the first task in this and all similar works is to argue that we have the objects we want to study.
Therefore, when we look to the literature of Ly$\alpha$ blob simulations and see that every other paper also reproduces blobs, there is call for suspicion.
We can't know from a literature search if methodologies that fail to reproduce blobs have been pursued, and only rarely to authors specifically offer evidence that their work is not sufficient to form blobs.
Therefore, we are left in an awkward position when comparing to other works because it is hard to tell if there are many ways to form a blob, or if all the ways to not form a blob have gone unreported (which is a known problem in other fields).
Looking back, we could have re-cast this entire project as a study of Ly$\alpha$ emission and escape from massive galaxies, as opposed to a body of work specifically on Ly$\alpha$ blobs.
We also could have attempted to present a strict and rigorous definition of a Ly$\alpha$ blob, but given the dispersion in physical properties of objects called blobs in the literature we would have almost certainly been presenting a definition that would redefine terminology used by other authors.
Whether or not this should be done anyway for the sake of clarity is a question left to the reader.

All that said, we will attempt to compare this work to previous attempts at studying the origin of Ly$\alpha$ in blobs.

In the broadest sense, this methodology used in this work was intended to be more grounded and thorough than other research on Lyman-$\alpha$ blobs.
Our primary improvements over other works are that we account for emission due to recombinations and collisional excitations, we compute rates for both those processes based on first principles, and we do Ly$\alpha$ Monte-Carlo radiative transfer on high(er)-resolution simulations.
We also include a recalculation of the ionization state in post-processing and use this ionization state calculation to insert an approximation for the effects of AGN.
That is to say, we account for all the fundamental physics that contribute to for Ly$\alpha$ emission and therefore can use those calculations to propogate our approximation of the effects of AGN into Ly$\alpha$ emission and escape.
To put this in the context of previous work, in the paragraphs below we comment on differences between previous works and our methodology.

Some of the previous work on LAB formation has been focused on emission from cooling gas, that is, collisionally excited neutral hydrogen.
\citet{Fardal2001} simulate only cooling emission and find luminosities of about the correct order of magnitude to be blobs, but would need to invoke yet-unquantified radiative transfer effects to explain the size.
\citet{Haiman2000} find that cooling emission is sufficient to reproduce the blobs mentioned in \citet{Steidel2000} (but their model is a simple analytical one).
\citet{Faucher-Giguere2010} also limit their model to cooling emission, but note that to reproduce the luminosities observed they need to model cooling emission from regions which should be experiencing star formation.
They do perform radiative transfer, and are able to reproduce the spatial extend of LAHs and the characteristic line profile of Ly$\alpha$ nebulae.
\citet{Rosdahl2012} account for emission from recombinations in an indirect manner; they treat it as a source of cooling, but are thus unable to discuss luminosity driven by recombinations.
\citet{Goerdt2010} also neglect emission from HII recombinations in their work.

By contrast, \citet{Cantalupo2005} and \citet{Gronke2017} omit cooling emission from their model, but include emission from recombinations.
We think this approach may be deeply problematic, since it appears to dominate for the no-AGN case, but \citet{Gronke2017} explicitly model AGN so we do not think this choice may be signifcant to their results.

Some papers in the literature account for both emission from cooling and recombinations \citep{Cen2013, Furlanetto2005, Geach2016, Smailagi2016}.
However, in all these papers the Ly$\alpha$ luminosity from recombinations is determined only by star formation rates, as opposed to our work where we use the age of star particles to inform stellar population synthesis and compute the ionization state of our gas.
\citet{Geach2016} account for the contribution of ionization from stars by locally modifying the ionization state of the gas based on the local star formation rate.

Some of the previous attempts at LAB modeling do not include Ly$\alpha$ radiative transfer effects \citep{Fardal2001, Furlanetto2005, Goerdt2010, Smailagi2016, Rosdahl2012}.
For the existing work that does include RT, it is often restricted due to low spatial resolution \citep{Cen2013} or discuss only a single line of sight \citep{Cantalupo2005}.
In this paper we demonstrate that there are strong line-of-sight variations, which can only be reproduced with reasonably accurate RT calculations.

\section{The Case for Radiation-Hydrodynamics}
The most uncertain part of this project is the ionization state and temperature of the gas, and most unfortunately that is the most out-of-scope improvement that could be made to this work.
The core problem is that the hydrodynamic simulations this work is based on do not include an accurate accounting for the effect of the UV background and the ionization of gas due to stars.
There are approximations for both of these, but they are very coarse and not self-consistent.
Specifically, the approximation for the cosmological UV background assumes that there is an ever-present UV field in the simulation, and thus ignores that the self-shielding effect of any neutral gas in the simulation against the cosmological UV background.
The approximation for stars is that a star will ionize some mass of gas near it, which while it does not ignore self-shielding entirely does tend to ignore radiative transfer effects that may cause highly asymmetric structure in the ionization state of the gas.
If there is one lesson from this work it is that radiative transfer rapidly becomes complex, so we are very wary of such an approximation.
The core of this problem is that the gas temperature and ionization state depend on each other, and both alter the Ly$\alpha$ emission rate equations.
In the case of an HII region, computing the extent of the ionization front requires doing ionizing radiative transfer through the gas, which depends on the temperature of the gas, but the ionizing radiative transfer ought to produce some heating of the gas.
That is, fixing these problems properly requires an on-the-fly treatment of the feedback loop between the heating of gas, resultant collisional ionization, radiative transfer of ionizing energy, radiative ionization, then re-calculation of the radiative transfer due to updated opacity of the gas.
Additionally, Ly$\alpha$ luminosity has a duplicate dependence on gas temperature because temperature feeds into the ionization state directly and also because the gas temperature alters the rate coefficients for both of the emission mechanisms.
Properly modeling this all is not computationally cheap, but may be critical for future more accurate Ly$\alpha$ studies.

To be clear, the work we have done here in post-processing is not a panacea; this \emph{must} be done on-the-fly because all computations done in post-processing will not be self-consistent.
In this work, we try to compute a more accurate ionization state at a timestep $t$ based on the physical conditions at $t$, but the ionization state will alter the evolution of the simulation.
Therefore, the physical conditions we base our ionization state at time $t$ upon are incorrect, and we actually needed to do this more accurate calculation at timestep $t-1$ and so on.

\section{Ly\texorpdfstring{$\alpha$}{a} Photon Tracking}
The widely-recognized advantage of simulations in astronomy is that we can conduct experiments, wheras in observations one must make do with whatever is present or visible on the sky.
There is another massive advantage though; in a simulation we can extract information which in observations is not just difficult but impossible to obtain.
The simplest exercise of this capability is to visualize exactly where the escaping flux originates from (which we do in \ref{sec:experiments}), but so much more work can be done in this area.
When we compute the escape fraction of a halo in recombinations or collisions, we do this by attributing each photon packet proportionally to the escape of each source based on the relative luminosities of the gas it originates in.
The same process could be done in an analysis of the origins of the escaping flux.
With sufficient changes to the ionization state calculation, one could even do the same process with our AGN-on simulations and attribute escaping flux to the presence of the AGN and correlate that to the originating gas properties or location.

We need not even limit ourselves to tracking photons once they escape; one could even modify {\sc colt} to extract information about the Ly$\alpha$ radiation flow through the gas in the simulation.
Note that this proposition is distinct from Ly$\alpha$ radiation pressure calculations that are being pursued using the methods presented in \citet{Smith2018}; while those focus in the effect that Ly$\alpha$ has on the gas, we would be interested in what gas cells are responsible (or disproportionately responsible) for the Ly$\alpha$ escape fraction or direction of radiation flow through the simulation.
This presents some computational challenges; the naive approach will not work because we cannot record the effect of every cell on every photon because that would rapidly grow the memory usage of the simulation, so we present a few less-precise alternatives.
Each octant in the simulation could record the luminosity-weighted direction of Ly$\alpha$ flow through it, by incrementing only two values at each scattering, and from this one could plot a Ly$\alpha$ radiation flow field, and thus exactly quantify the resonant scattering interactions we explored previously in this work.
Specifically, one could answer definitively if the effect of AGN in this model is to create low-opacity pathways through the simulation through which Ly$\alpha$ that would escape along a different direction is scattered into and thus out of the halo.


\section{Verification of Analytical Prescriptions}
Some previous Ly$\alpha$ papers use approximations for the luminosity of a simulation or the escape fraction based on a number of other factors.
Since we have a more detailed simulation, we could check whether the outputs of our simulation agree with theirs since we have both the same inputs they used to predict the luminosity and the escape fraction and our more precise values for those quantities.
Most notably \citet{Cen2013} use the star formation rate to compute the luminosity of the gas based on a relation that originates with a much older stellar population synthesis model and an approximation for conversion of ionizing radiation to Ly$\alpha$ that is suspiciously simple.
The rate of case-B recombinations (eq. \ref{eq:j_rec}) is a function of the temperature, density, and ionization state of the gas but this relation they rely on smooths over all those factors.
We could verify this approximation on a case-by-case basis or over the simulation overall, and if we find a discrepancy all our simulations could be re-run using this approximation, to determine if this approximation is the cause of the discrepancy in our results.

Additionally, a number of papers avoid doing Ly$\alpha$ radiative transfer, and since this is the most computationaly expensive part of this work it would be very valuable to future work if one could implement and possibly validate an approximation.


\section{Efficiency of Numerical Approximations}
In an number of places in {\sc colt} we rely on numerical approximations of analytical functions, some of which originate some time ago.
The throughput of {\sc colt} is limited in cases where we produce only one surface brightness image by the throughput of the accept/reject algorithm that produces a scattering direction.
If the throughput of radiative transfer simulations is a limiting factor in resesarch progress (which we believe it is), we should revisit the implementations used, and possibly apply some modern tooling to these algorithms.
Specifically, the throughput of the simulation is primarily bottlenecked by triganometric functions, which are used in approximations of integrals.
It is an open question whether we could replace these with faster but less accurate approximations; the generic math function implementations provided by a standard library are required to provide a level of precision that probably we do not require for these simulations.

Additionally, the ratio of compile time (less than a minute) to CPU time runnning ($\sim1,000$ hours per snapshot) makes this progject a prime candidate for a superoptimizer.
Wheras the optimizer part of a normal compiler has to balance time spent compiling and efficiency of the generated code and therefore uses a large number of heuristics, a superoptimizer attempts to discover the exactly optimal sequence of instructions to accomplish a task.
The particular superoptimizer that may be applicable to this work is {\sc stoke} \citep{stoke}, which could be used to derive new and much faster approximations for all the approximations used in this work.


\section{Image-oriented Optimizations/How much does gas velocity matter, anyway?}
In {\sc colt} the creation of surface brightness images is produced by next event estimation which is a per-line-of-sight process done at each scattering event.
This next event estimation is done by walking the MCRT photon packet along each line of sight we want to generate surface brightness images along, and as it passes through each octant the intensity is reduced by the optical depth of the path length, and the frequency of the photon packet is Doppler shifted according to the difference in the velocity of the gas in each octant.
If we only generate one surface brightness image, the scattering event calculation dominates.
If we generate surface brightness images along 12 lines of sight, approximately equal time is spent in the next event estimation and scattering code.
{\sc colt} has another mode wherein it generates 360 surface brightness images that can be stiched together into a video which shows a view from all lines of sight around the $z$ axis; in this mode the runtime is entirely dominated by the Doppler shift calculation between octants.
Perhaps more importantly, we have used nearly as much CPU time in this mode to generate a handful of videos as we have on running all the other radiative transfer calculations in this project.
Nearly all of this CPU time is spent Doppler-shifting the photon packet as it moves from cell to cell; this calculation is technically required as the opacity of the Ly$\alpha$ line is dependent on the relative velocity of the gas in the cell, but we do not have any proof or demonstration if this Doppler shift significantly alters the outcome of the next event estimation.
Testing this would not be particularly difficult, we could render a number of surface brightness images with the Doppler shifting code removed from the simulation.

If the Doppler shift is required, we could look for other ways to shortcut the next event estimation code.
At startup, {\sc colt} uses a fast estimation of the probability of a photon directly escaping the simulation from each octant.
We may be able to use this information or a slightly more detailed version to abort the next event estimation more quickly; currently the next event estimation only stops once the photon packet has done all the Doppler shifting and has been sufficiently attenuated.
The consequence of this is that next event estimation for lines of sight that do not make it out of the simulation are maximally computationally expensive.
We should be able to do better, possibly by recording some information about the range of $a\tau$ values visible from each octant.

Alternatively, it may be possible to produce approximate surface brightness images along many lines of sight by recording approximate luminosity distributions in each cell during the normal Ly$\alpha$ scattering calculations, then projecting from every cell onto the surface brightness images at once.
In this scheme, the calculation time for the surface brightness images would not scale with the product of the number of Ly$\alpha$ scatterings and the depth of those scatterings, and instead just scale with the number of cells in the simulation.
The issue with this is (as ever) whether it is possible to balance the space usage of the additional luminosity distribution in each cell with the time savings in rendering all the surface brightness images.


\section{Convergence in Radiative Transfer Simulations}
To work out how many MCRT photons packets were required to produce usable surface brightness images, we ran simulations with varying numbers of photon packets until we found that the contours plotted in Figure~\ref{fig:area_plot} were converged to the surface brightness limit of observations; see Figure~\ref{fig:j_exp_profiles} for proof of convergence to most sensitive observations.
This experiment is the origin of the $10^7$ number that has been quoted elsewhere in this work
This $10^7$ number is a reasonable compromise between runtime (2,000 CPU-hours per snapshot) and the reliability of our contours which we rely on as evidence of LAB formation, but is fundamentally unsatisfying since it is so intimately tied to the science objectives of this research.
We do not have precise, satisfying criteria for the convergence of the ionization state when we run {\sc lycrt} either, which since it is evolving the state of the gas has multiple parameters that can be used to adjust convergence (number of iterations, number of photons emitted from stars per iteration, number of UV background photons per iteration).
In this project, we ran our Ly$\alpha$ radiative transfer at each iteration of the ionization solver, verifying that after just a few iterations, the run-to-run variation in the ionization state calculation disappears into the variance of the Ly$\alpha$ radiative transfer calculation.
This provides us some assurance of correctness, but does not help in extending this work to other problem domains (what if we want to do this on larger, smaller, or very early/late universe simulations) and does not help us make responsible use of compute resources.
That is, we know that our ionization state calculation is sufficient, but we do not know if all the work was necessary (and suspect it wasn't).

These procedures we adopted are are fundamentally unsatisfying for multiple reasons.
Most importantly, they are not necessarily reproducible.
In general, astronomy has a problem with the reproducability of our analysis and magic numbers like this certainly aren't helping.
Additionally, we do not have any guarantee that these outputs are {\emph actually} converged.
It's very unlikely that the results presented in this paper are meaningfully incorrect due to convergence issues, because all the code used here has been re-run for a variety of reasons and the plots do not seem to shift at all.
But even that is a claim of reproducability, not convergence.
We also know from experience running these simulations that the intensity plotted in our Ly$\alpha$ surface brightness images does not converge uniformly.
As the simulation runs, the surface brightness of dimmer regions becomes brighter, and sometimes in a manner that is visually obvious.
Therefore, it is possible that though we have run the code a few times and gotten results that are similar, that surface brightness profiles (or some equivalent) that we compute are too steep, and systematically so because we do not have (or have an incorrect) metric for convergence.

But lastly and perhaps most importantly, what we want is a setting that can be toggled on the simulation or in the code that simply says ``run however long it takes to get a converged result''.
The problem with this is that we need to very clearly define what number we want to be converged, and what tolerance is acceptable.
We can see nods to this requirement in various literature; for example \citet{Smith2015} justify their core-skipping methodology and approximation of the Hjerting-Voigt function on the basis that they are better than 0.1\% and 1\% respectively.
In this framework, we could consider a few metrics for convergence that a user of a codebase may wish to tune.
The simplest and fastest-converging metric is the total escape fraction; the only problem with this is that most of this work has been a demonstration that the mean escape fraction is much less interesting than other properties of Ly$\alpha$ escape.
We could also place requirements on the median escape fraction; while this would provide a good metric for how accurate this blob is for a typical observation of it much of the interesting work we have presented here requires more complex metrics.
To ensure convergence of many of the the plots presented here, we would need to constrain the convergence of the $1$, $2$, and $3\sigma$ escape fraction values.
The danger with requiring constraint on the $3\sigma$ values of any distribution is that they will be unconverged for much longer (because they are sensitive to a small number of samples), so perhaps such a scheme would have to consider a reasonable usage of compute time.
In a very similar vein, we would like to also constrain the convergence of our surface brightness images.
However, we already know that the brighter pixels converge first (and our existing luminosity boosting does relatively little to address this) so the choice of at what surface brightness level we require some particular tolerance is absolutely critical.
For this project at least, none of our surface brightness images (even the ones with $10^{9}$ photon packets which ran for $\sim25,600$ CPU-hours) are entirely converged and therefore we suggest that future work should try to pin their surface brightness convergence to the best currently achieved by observations, or likely capability in the upcoming few years.

\section{Software Future Directions}

It's generally accepted that most code in the world is vastly slower than it could be, even if it's written by experts in a language like C++.
But software in astronomy is somewhat of an extreme case, for a number of reasons.
With the growth of data in the field (and especially in this project) a number of strategies that were technically sub-optimal but not a serious issue are heading towards that.
In this project there are two specific examples: Numerical tables stored in text files, and eager initialization of large data structures.
Storing data in a text format is a very attractive solution for data interchange; unlike all other formats if documentation is lost entirely at least the data itself can be recovered (if not the semantics thereof).
But text files are rather slow to read, and the common tools for reading and writing them are embarrassingly slow. To be clear, we are referring to \lstinline{fprintf} and \lstinline{fscanf} and in the Python world, \lstinline{numpy.savetxt} and \lstinline{numpy.loadtxt}.
A disappointing (but not large in an absolute sense) amount of time in this project was spent waiting for text files to save or load.
There are partial solutions to this problem; reading and writing of text formats can be made much faster.
For example \lstinline{loadtxt} (available at \lstinline{https://github.com/saethlin/loadtxt}) can load text files much faster than tools in \lstinline{numpy}, and a similar approach could be taken using Ulf Adams's ryu algorithm for saving text files (though as with code, reading is the more common operation).

The HDF5 format is a much better solution to rapidly reading data from files.
It's fast enough, supports transparent compression, and allows adding metadata to files and datasets within a file.
This could even be used to associate units with data, which is invaluable in a field where there are multiple reasonable units for any quantity.

The one downside to the HDF5 format is that there is a single implementation, a C library which is produced by the HDF group, which is not without flaws.
The original plan for this project included a study of the impact of subhalos on Ly$\alpha$ blob formation, but the halo finder that the research group uses (and this project relies upon) does not have information about subhalos because it relies on a simple 3d friends-of-friends implementation.
The planned fix for this was to incorporate Amiga's Halo Finder (AHF), but at the time AHF did not support loading snapshots from HDF5 files.
We attempted to add such support from AHF, but were stymied by problems with the HDF5 library.
The interface in AHF for adding input file support is rather complex and involves a lot of type erasure through \lstinline{void*}, so we tried to build and test the additions with the sanitizers.
The sanitizers are a set of compiler flags supported by clang and gcc (sometimes) which add a large number of runtime checks to a program to look for things like integer overflow, out-of-bounds array acceses, invalid pointer dereferences, use of uninitialized memory, and data races.
Unfortunately we were unable to test our additions to AHF with memory sanitizer, because it detected a use of uninitialized memory in the intiializaion code for the HDF5 library.
While two of the sanitizers (undefined behavior sanitizer and thread sanitizer) are able to continue execution of the program even if it is invalid by producing a sensible default, memory sanitizer is not.
This problem (or problems) with the HDF5 library are more widespread than just this effort, it effectively means that since there is a bug in the HDF5 library we are unable to use a very high-quality tool to automatically discover similar bugs in any software that uses the HDF5 library.
Ideally, in this project we could have patched the HDF5 library but that codebase is very hard to approach because the format it is dealing with has some idiosyncracies, and because the HDF5 library makes very heavy use of unhygienic macros.

\section{Better Octrees}

Much of this work involves octrees, which are a rather common data structure in simulations in general.
While we use them as an adaptive grid, they're often also used as a way to look up nearest neighbors in a particle simulation.
Navigating an octree does not dominate the runtime of \textsc{colt}, but it does dominate the runtime of \textsc{lycrt}, and it can dominate the runtime of some other codes like \textsc{gizmo} when additional self-gravitating particles are added.
For this reason, one might naively expect that there exists a highly-polished implementation of an octree that is used ubiquitously, or at least that the data structure is well-studied enough that the most optimal implementation strategies are well-established.
Unfortunately, this does not seem to be the case.
We include below, the approximate definition of the octant structure used in \textsc{lycrt} (it has been cleaned up a bit).
\begin{lstlisting}
struct Cell {
    double min_x[3];
    double width;

    long parent_ID;
    long sub_cell_check;
    long sub_cell_IDs[8];

    LOCALVAL *U;
};
\end{lstlisting}
For comparison, here is a similarly cleaned-up version of the octant structure used in the public \textsc{gizmo} codebase (similarly cleaned up).
\begin{lstlisting}
struct Cell {
  MyFloat center[3];
  MyFloat len;

  union {
    int suns[8];
    struct {
      int father;
      MyFloat s[3];
      MyFloat mass;
    } d;
  } u;
};
\end{lstlisting}
These two implementations both contain the same basic ingredients; the location of the octant, its size, some information required to navigate the tree structure, and the actual data that the octree carries(gas properties or a mass-only particle).
Observe that \emph{in principle} an octree domain could be any rectangular prism, and in that case we would position for each octant, and there would be three side lengths.
But in both these implementations, the implementors make the simplification that octants are cubic and therefore only one side length is required.
There is opportunity for further simplification.

In an octree, each level down has dimensions that are half that of the parent level in the tree.
That is, given an octree with $n$ levels where the root of the tree is at level $n-1$ and the most-refined cells are at level $0$, the side length $L$ is given by $L(i+1) = 2L(i)$.
In simpler terms, for every step up the tree the side length doubles.
Therefore, we can compute the side length of any octant as $L(i) = 2^{i} L(n)$.
This level numbering scheme is cumbersome to use when building an octree because the level number of any cell depends on how deep the tree is refined, but the more obvious scheme wherein the root is level $0$ could be used when building the tree, then the scheme could be inverted once the number of levels in the tree is known.

The benefit to this level numbering is that we can store the side length of an octant with a single byte.
In a single byte, we can store numbers up to $255$ which may not seem like an enormous amount of refining power, but because this is an exponent just one byte lets us represent octree divisions that cover $10^{76}$.
We could even steal the most significant byte for some other use and still have $10^{38}$ left to work with, which is probably sufficient for most applications.
And even if $76$ orders of magnitude does not suffice for some application, stealing another byte for the octree level permits representing more resolution than can be stored in a 64-bit IEEE floating point.

The greatest benefit to this approach is not just in reducing the in-memory size of data structures, it is in permitting a much more memory-dense octree implementation.
The primary limiter to the runtime of programs on a modern CPU is the latency of memory access, and compacting data structures in memory can provide order-of-magnitude runtime improvements when a program needs to walk over the data structure (which is the whole reason octrees exist, to be walked).
But this approach, while an improvement in memory size carries a downside in that if we are given a coordinate in space, to find out if it belongs to a tree we need additional operations.
We need to reconstitude the width of an octant to compare it to an absolute position:
\begin{lstlisting}
double width(Ocatant* oct) {
    return Tree.minimum_octant_width * (1 << oct->level);
}
\end{lstlisting}

At first this seems quite unfortunate, but there is another symmetry observation about octrees that saves us.
The finest octree refinement level can serve as our base not only for side length, but also for the spatial coordinates of an octree, particularly if we store the coordinates of one corner as does {\sc lycrt}.
For the octrees used in this work, the position of the octree cells could just barely be stored in a 16-bit integer, but in general it would probably be responsible to use at least 32 bits for this.
At first, this change may also seem like yet more work to convert the coordinates of an octant into floating-point to compare to something outside.
However, if we consider the operation which is common to {\sc lycrt} which is ``Given this location in space, which cell does it belong to'', we could convert that coordinate in space into our integer octree coordinates, then search our now-compacted octree using only integer operations.
We propose this octant structure:
\begin{lstlisting}
struct Cell {
    uint8_t level;
    uint32_t position[3];
    bool has_children;
    union {
        long child_IDs[3];
        GasProperties gas;
    } contents;
};
\end{lstlisting}
